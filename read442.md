# Ethics in workspace

## The code I’m still ashamed of

If you write code for a living, there’s a chance that at some point in your career, someone will ask you to code something a little deceitful – if not outright unethical.

This happened to me back in the year 2000. And it’s something I’ll never be able to forget.

I wrote my first line of code at 6 years old. I’m no prodigy though. I had a lot of help from my dad at the time. But I was hooked. I loved it.

By the time I was 15, I was working part-time for my dad’s consulting firm. I built websites and coded small components for business apps on weekends and in the summer.

I was woefully underpaid. But as my dad still likes to point out, I got free room and board, and some pretty valuable work experience.

Later, I managed to help fund a part of my education through a few freelance coding gigs. I built a couple of early e-commerce sites for some local small businesses.

By age 21, I managed to land a full-time coding job with an interactive marketing firm in Toronto, Canada.

The firm had been founded by a medical doctor and many of its clients were large pharmaceutical companies.

In Canada, there are strict limits on how pharmaceutical companies can advertise prescription drugs directly to consumers.

As a result, these companies would create websites that present general information about whatever symptoms their drugs were meant to address. Then, if a visitor could prove they had a prescription, they were given access to a patient portal with more specific info about the drug.

## The employee backlash over Google’s censored search engine for China, explained

Google is experiencing a “moral and ethical” crisis. That’s the view of hundreds of employees at the tech company, who are protesting the development of a censored search engine for internet users in China.

About 1,400 Google employees — out of the more than 88,000 — signed a letter to company executives this week, seeking more details and transparency about the project and demanding employee input in decisions about what kind of work Google takes on. They also expressed concern that the company is violating its own ethical principles.

“Currently we do not have the information required to make ethically-informed decisions about our work, our projects, and our employment,” they wrote in the letter, obtained by the Intercept and the New York Times.

The existence of the censored search tool — dubbed Dragonfly — was revealed earlier this month by the Intercept, sparking outcry within the company’s ranks and drawing harsh criticism from human rights groups across the world. Internal documents leaked to journalists described how the app-based search platform could block internet users in China from seeing web pages that discuss human rights, peaceful protests, democracy and other topics blacklisted by China’s authoritarian government.

Only a small group of Google engineers are reportedly developing the platform for Beijing, and information about the project has been so heavily guarded that only a few hundred Google employees even knew about it. Google has declined to comment publicly on Dragonfly, but Google CEO Sundar Pichai defended the project Thursday during a weekly staff meeting, saying that the project for China is merely in the “exploratory” stage.

The internal backlash among employees represents mounting concerns about whether Google has “lost its moral compass” in the corporate pursuit to enrich shareholders. But it also suggests that the people who make Google’s technology have more power in shaping corporate decisions than even shareholders have. In April, thousands of Google employees protested the company’s military contract with the Pentagon — known as project Maven — which developed technology to analyze drone video footage that could potentially identify human targets.

Illustration of a crowd of identical blank profile outlines, with one in red blowing a whistle.

## Google Backtracks, Says Its AI Will Not Be Used for Weapons or Surveillance

Google is committing to not using artificial intelligence for weapons or surveillance after employees protested the company’s involvement in Project Maven, a Pentagon pilot program that uses artificial intelligence to analyze drone footage. However, Google says it will continue to work with the United States military on cybersecurity, search and rescue, and other non-offensive projects.

Google CEO Sundar Pichai announced the change in a set of AI principles released today. The principles are intended to govern Google’s use of artificial intelligence and are a response to employee pressure on the company to create guidelines for its use of AI.

Employees at the company have spent months protesting Google’s involvement in Project Maven, sending a letter to Pichai demanding that Google terminate its contract with the Department of Defense. Several employees even resigned in protest, concerned that Google was aiding the development of autonomous weapons systems.

Google will focus on creating “socially beneficial” AI, Pichai said, and avoid projects that cause “overall harm.” The company will accept government and US military contracts that do not violate its principles, he added.

# Ethics in Technology

## The ethical dilemmas of self-driving cars

as self-driving technology creeps into vehicles, the debate over how they should behave in situations they can't anticipate is sharpening.

Multiple studies estimate that autonomous cars would dramatically reduce road accidents – up to 90 per cent, according to a 2015 report by McKinsey & Company.

No one believes accidents will be eliminated entirely, which brings up an ethical dilemma: Who should the car harm if it finds itself in one of those unavoidable situations? Do children, elderly people or other factors change the equation?

"Nobody's talking about ethics," Ford Motor Co. chairman Bill Ford said a year ago, speaking with The Globe and Mail's Greg Keenan and a small group of reporters. "If this technology is really going to serve society, then these kinds of issues have to be resolved, and resolved relatively soon."

We already have the technology.

"The greater challenge is the artificial intelligence behind the machine," Toyota Canada president Larry Hutchinson said, addressing the TalkAuto Conference in Toronto last November. "Think of the millions of situations that we process and decisions that we have to make in real traffic. … We need to program that intelligence into a vehicle, but we don't have the data yet to create a machine that can perceive and respond to the virtually endless permutations of near misses and random occurrences that happen on even a simple trip to the corner store."

Consider, he said, the ethical dilemma that an autonomously driven car would need to resolve in an instant when a child jumps suddenly into the car's path from the curb. There's no time to brake. What then? Veer left into oncoming traffic, possibly causing an accident that would injure the driver and passengers? Veer right onto the sidewalk, possibly injuring pedestrians? Continue straight, possibly colliding with the child?

Germany last year became the first country to attempt to answer such morbid questions with actual guidelines. The proposed rules state that self-driving cars should always attempt to minimize human death and shouldn't discriminate between individuals based on age, gender or any other factor. Human lives should also always be given priority over animals or property.

More than three-quarters of participants support such a utilitarian approach, according to a 2016 study published in Science. That is, unless they happen to be in such a car, in which case the majority would want it to protect passengers – and themselves – at all costs.

This revelation suggests the issue isn't solely about ethics, but rather it's also about control. People think ethically in principle, but in practice they behave more selfishly, especially if they are not in command of a situation.

